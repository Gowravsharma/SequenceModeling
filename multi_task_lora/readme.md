# ğŸ“˜ Multi-Task LoRA Adapters for BERT

This project demonstrates how to train **multiple LoRA adapters** on different NLP tasks using a shared **BERT-base** model.  
Two separate adapters are trained:

- **Sentiment Analysis** (SST-2 â€“ GLUE Benchmark)  
- **Paraphrase Detection** (QQP â€“ GLUE Benchmark)

A simple **routing mechanism** selects the correct adapter at inference time, enabling modular skill composition.

---

## ğŸš€ Features
- Fine-tune **task-specific LoRA adapters** using the PEFT library  
- Modular loading of adapters on top of a shared backbone  
- Dynamic routing based on input query  
- Evaluation scripts for both tasks  
- Clean inference API (`predict(text)`)

---

## ğŸ“ Project Structure
```
multi-task-lora/
â”‚â”€â”€ train_sentiment.py
â”‚â”€â”€ train_paraphrase.py
â”‚â”€â”€ adapter_loader.py
â”‚â”€â”€ inference.py
â”‚â”€â”€ evaluate.py
â”‚â”€â”€ adapters/
â”‚    â”œâ”€â”€ sentiment/
â”‚    â””â”€â”€ paraphrase/
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ __init__.py
```

---

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install torch transformers datasets peft accelerate bitsandbytes
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training

### 1. Train Sentiment LoRA (SST-2)

```bash
python train_sentiment.py
```

Outputs LoRA weights in:

```
adapters/sentiment/
```

### 2. Train Paraphrase LoRA (QQP)

```bash
python train_paraphrase.py
```

Outputs:

```
adapters/paraphrase/
```

---

## ğŸ”§ Inference

Use the unified `predict()` method:

```python
from inference import predict

print(predict("This movie was awesome!"))
print(predict("Are these two questions asking the same thing?"))
```

Example output:

```
Using adapter: adapters/sentiment
POSITIVE

Using adapter: adapters/paraphrase
duplicate
```

---

## ğŸ”€ Dynamic Adapter Routing

```python
def route_adapter(prompt):
    if "similar" in prompt or "paraphrase" in prompt:
        return "adapters/paraphrase"
    return "adapters/sentiment"
```

---

## ğŸ“Š Evaluation

```bash
python evaluate.py
```

Output example:

```
SST-2 Accuracy: 0.92
QQP Accuracy: 0.88
```

---

## ğŸ§  Concepts Demonstrated
- **PEFT (LoRA, QLoRA-ready)**  
- **Multi-task adapter training**  
- **Modular skill composition**  
- **Efficient fine-tuning of Transformer models**  
- **Dynamic task routing**  

---

## ğŸ“œ License
MIT License.
